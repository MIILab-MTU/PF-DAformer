# wandb parameters
project: qct_seg
wandb_parameters:
  id: 5aj33zof  #id of your wandb run # mostlky used while resuming training 
  mode: "online" # set this to "online" if you want to log to wandb #else set to offline
  entity: rochakdh-michigan-technological-university
  group: QCT
  name: ABS_TranUNET_DA_loss_F_0.6_D_0.4_0.1_0.1_v100_again  #Change name for wandeb setup
  resume: False  #keep this false if you are using fresh trainig 
  tags: ["rochakdh-michigan-technological-university", "dice", "b0_model", "adamw"]

# model parameters
model_name: trans3dUNET #segformer3d, trans3dUNET , swin3d ,nnfo
model_parameters:
    in_channels: 1
    sr_ratios: [4, 2, 1, 1]
    embed_dims: [64, 128, 256, 384]            # â†‘ Balanced increase
    patch_kernel_size: [5, 3, 3, 3]
    patch_stride: [4, 2, 2, 2]
    patch_padding: [2, 1, 1, 1]
    mlp_ratios: [4, 4, 4, 4]
    num_heads: [2, 4, 8, 12]                   # Matches embed_dims scaling
    depths: [3, 3, 3, 2]                       # Same depth for consistency
    num_classes: 1
    decoder_dropout: 0.15                      # Slightly stronger regularization
    decoder_head_embedding_dim: 384   

# loss function
loss_fn:
  loss_type: "dice"
  loss_args: None

# optimizer
optimizer:
  optimizer_type: "adamw"
  optimizer_args:
    lr: 0.0001
    weight_decay: 0.01

# schedulers
warmup_scheduler:
  enabled: True # should be always true
  warmup_epochs: 38

train_scheduler:
  scheduler_type: 'cosine_annealing_wr'
  scheduler_args:
    t_0_epochs: 100
    t_mult: 1
    min_lr: 0.000006

# (Not fully implemented yet) eponential moving average
ema:
  enabled: False
  ema_decay: 0.999
  val_ema_every: 1

sliding_window_inference:
  sw_batch_size: 1
  roi: [192, 192, 192]

# gradient clipping (not implemented yet)
clip_gradients:
  enabled: False
  clip_gradients_value: 0.1

# training hyperparameters
training_parameters:
  seed: 42
  num_epochs: 300
  cutoff_epoch: 150
  load_optimizer: False
  print_every: 200
  calculate_metrics: True
  grad_accumulate_steps: 1
  checkpoint_save_dir: "model_checkpoints/best_dice_checkpoint"  # directory to save .pth files
  calculate_metrics: True
  grad_accumulate_steps: 1
  load_checkpoint:
      load_full_checkpoint: True       # True to resume full model + optimizer #During Fresh training keep it False. To resume Training keep if True
      load_model_only: False            # True for finetuning from pretrained weights  #Not Implemented Properly so keep it Flase
      load_checkpoint_path: /workspace/QCT_Segmentation/pfdaformer/experiments/exp0/template_experiment/model_checkpoints/best_dice_checkpoint/best_dice_model_post_cutoff/    # path to .pth checkpoint file (e.g. "checkpoints/checkpoint.pth")
      # There are two foler before and after cutoff pelase make ure you are uisng correct one
      
dataset_parameters:
  dataset_type: "qct_seg"
  datasets:
    - name: "source"
      train_dataset_args:
        root: "../../../data/tulane_qct"
        train: True
        fold_id: null
      val_dataset_args:
        root: "../../../data/tulane_qct"
        train: False
        fold_id: null

    - name: "target"
      train_dataset_args:
        root: "../../../data/uci_qct"
        train: True 
        fold_id: null
      val_dataset_args:
        root: "../../../data/uci_qct"
        train: False
        fold_id: null

  train_dataloader_args:
    batch_size: 1
    shuffle: True
    num_workers: 0
    drop_last: True

  val_dataloader_args:
    batch_size: 1
    shuffle: False
    num_workers: 0
    drop_last: False


evaluate_parameters:
  checkpoint_path: "/workspace/QCT_Segmentation/pfdaformer/experiments/exp0/template_experiment/model_checkpoints_ABS_TranUNET_DA_loss_F_0.6_D_0.4_0.1_0.1_v100_again/best_dice_checkpoint/best_dice_model_post_cutoff/checkpoint.pth"
  csv_path: "/workspace/QCT_Segmentation/pfdaformer/data/uci_qct/train.csv"

  # Inference options
  save_predictions: True  # Set True to save predicted masks

  # Loss function setup (must match training config)
  loss_fn:
    loss_type: dice   
    loss_args: {}

  sliding_window_inference:
    sw_batch_size: 1
    roi: [192, 192, 192]
